
# Behavioral Cloning Project

---

The goals / steps of this project are the following:

- Use the simulator to collect data of good driving behavior
- Build, a convolution neural network in Keras that predicts steering angles from images
- Train and validate the model with a training and validation set
- Test that the model successfully drives around track one without leaving the road
- Summarize the results with a written report

### 1. Required files

- model.py: the implementation of the model architecture
- preprocess.py: used for preprocessing and augmenting images before training process
- drive.py: slightly modifed version of the original file (in order to preprocess the images)
- model.h5: keras model
- video_one.mp4: driving on track one
- video_two.mp4: driving on track two

### 2. Code

For easier experimenting the code was splitted into two files: model.py and preprocess.py. Thanks to that, preprocessing (including data augmentation) may happen separately from actual training. Once the preprocessing is done, the main focus may go to tuning hyperparameters of the model. Running the code is as simple as:


```python
python preprocess.py # reads from data folder and generates x_train.npy, y_train.npy
python model.py # executes the actual training and generated model.h5
```

I used quite powerful machine for training (please see my <a href="https://www.tooploox.com/blog/deep-learning-with-gpu">blogpost</a> about it), so using generators wasn't quite necessary. However to meet requirements I implemented the code (see preprocess.py), I didn't use it (commented out), as it slows down the training process.

The code of drive.py was also modified - the speed was increased, and the input to the model is now preprocessed.

### 3. Model Architecture and Training Strategy

##### 3.1 Architecture

The architecture, which was used in this project, resambles the one from <a href="https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/">Nvidia</a> (click the image below to enlarge). 

<img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/model.png" style="height: 500px"/>


In order to improve learning performance and overall model accuracy <b>batch normalization was introduced to the above model</b>. There is also <b>elu non-linearity</b> used as activation function. Please refer to <a href="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/complete_model.png">this file</a> to have a look at the complete architecture.

The input for the network differs from the input generated by the simulator, therefore resizing to desired shape was required. It happens both in preprocess.py, where training data is resized, and in drive.py, where testing data is resized. Also a conversion from BGR (training), RGB (testing) to YUV was made, to follow Nvidia's paper. Each input is also normalized, in the following way:


```python
# x is the input image of input_shape
model.add(Lambda(lambda x: (x / 127.5) - 1.0, input_shape=(66, 200, 3)))
```

##### 3.2 Data collection

In order to collect data for training set, the following strategy was applied:
- drive 3 laps in the middle of the road clockwise
- drive 3 laps in the middle of the road counter-clockwise
- no extra recovery cations were recorder

This steps were repeated both for track one and track two.

<table>
<tr>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/original.png"/></td>
<td>Original image</td>
</tr>
<tr>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/cropped.png"/></td>
<td>Region of interest</td>
</tr>
</table>

Moreover, images coming from all 3 cameras was used, with a correction of 0.2 magnitude.

<table>
<tr>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/left.png"/></td>
<td>Image from left camera (correction 0.2)</td>
</tr>
<tr>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/center.png"/></td>
<td>Image from center camera</td>
</tr>
<tr>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/right.png"/></td>
<td>Image from right camera (correction -0.2)</td>
</tr>
</table>

##### 3.3 Data augmentation

To make the training dataset more various, there were several augmentation strategies applied:
- flip the image horizontally
- adjust the brightness randomly
- add shadow randomly
- shift the horizon

The first two approaches are self-explanatory. The third one, makes a rectangular part of the image darker, than the remaining part. The last one is not really necessary for the first track, but is quite beneficial in case of the second one - it allows to emulate up-hill/down-hill roads. 

<table>
<tr>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/flipped.png"/></td>
<td>Flip horizontally</td>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/brightness.png"/></td>
<td>Adjust brightness randomly</td>
</tr>
<tr>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/shadow.png"/></td>
<td>Add shadow randomly</td>
<td><img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/horizon.png"/></td>
<td>Shift horizon</td>
</tr>
</table>


All of the mentioned augmentation techniques are implemented in the preprocess.py file. The final preprocessing distribution looks as follows:

<img src="https://raw.githubusercontent.com/joekidd/CarND-Behavioral-Cloning-P3/master/examples/t_distr.png"/>

Most of the steering measurements are from the range(-0.25, 0.25), what might be easily explained by doing small corrections to the driving trajectory while driving. There is also more examples around -1 and 1, as they were generated by adding the correction value, while using images from left/right camera. In total, I managed to get <b>404892 examples</b>, that were created from just 12 full circuits made on both tracks.

##### 3.4 Training strategy and avoiding overfitting

The collected data was splitted into two parts: 60% was taken as a training set, 20% was left to be used for validation and the remaining 20% as a test set. Early stopping was implemented and the training process was stopped, when validation loss didn't improve within 3 epochs.

In order to train the network, Adam optimizer was used with MSE as the loss function.

Because <b>batch normalization was used, there was no need to use dropout, or strong l2 weight normalization</b>.

### 4. Simulation

There are video_*.mp4 files presenting the results of the trained models on both tracks. GIFs below link to youtube videos presenting whole circuits.

##### 4.1 Track one
<a href="https://youtu.be/9yHRG9Ir63M">
<img src="https://github.com/joekidd/CarND-Behavioral-Cloning-P3/blob/master/examples/track_one.gif?raw=true"/>
</a>

##### 4.2 Challange
<a href="https://youtu.be/Y4ETFuCkmOE">
<img src="https://github.com/joekidd/CarND-Behavioral-Cloning-P3/blob/master/examples/challange.gif?raw=true"/>
</a>  
  
### 5. Conclusions

Having enough data is crucial in applications of deep learning methods. Even if the project contains only 2-tracks and the world in the simulator is way simpler than the real road conditions, there is still a need for having a good training set. Luckily, strong data augmentation methods come in help and allow to avoid long data collection process. Ultimately, I got numbers of various examples, which are hard to overfit to.

Batch normalization is a good improvement of the architecture proposed by Nvidia and makes the model to converge faster. 

As a further work I could try to train the network to predict not only steering commands, but also break and throttle.


```python

```
